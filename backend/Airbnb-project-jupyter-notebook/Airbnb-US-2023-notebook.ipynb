{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./vehicles.csv\")\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(cars)}\")\n",
    "clean_cars = cars.drop([\"id\", \"url\", \"region\", \"region_url\", \"VIN\", \"image_url\", \"description\", \"county\", \"state\", \"lat\", \"long\", \"posting_date\"], axis=1)\n",
    "cars.rename(columns={\n",
    "    \"year\": \"entry_year\",\n",
    "    \"title_status\": \"vehicle_status\",\n",
    "    \"size\": \"vehicle_size\",\n",
    "    \"type\": \"vehicle_type\"\n",
    "})\n",
    "clean_cars = clean_cars.drop_duplicates()\n",
    "print(f\"Length after removing duplicates: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ea78c",
   "metadata": {},
   "source": [
    "### Removing all rows that describe the same car but different price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c79142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(clean_cars)}\")\n",
    "rows_to_remove = clean_cars[clean_cars.drop(\"price\", axis=1).duplicated(keep=False)].index\n",
    "clean_cars = clean_cars.drop(rows_to_remove, axis=0)\n",
    "print(f\"Length after removing same cars different price: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab5b1b",
   "metadata": {},
   "source": [
    "### Dealing with missing values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbad171",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cars.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_null_values_count_per_column(dataframe):\n",
    "    end_output = \"\"\n",
    "    for column in dataframe.columns:\n",
    "        end_output += f\"{column}: {len(dataframe[dataframe[column].isnull()])},\\n\"\n",
    "    end_output = end_output.rstrip(\",\\n\")\n",
    "    print(end_output)\n",
    "\n",
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car model is essential for predicting price, thus null values are dropped\n",
    "no_nulls = clean_cars.copy()\n",
    "no_nulls = no_nulls.dropna(subset=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nulls[no_nulls.model.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd02a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_nulls.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_cars[clean_cars.year.isnull()]))\n",
    "print(len(clean_cars[clean_cars.manufacturer.isnull()]))\n",
    "print(len(clean_cars[clean_cars.model.isnull()]))\n",
    "print(len(clean_cars[clean_cars.condition.isnull()]))\n",
    "print(len(clean_cars[clean_cars.cylinders.isnull()]))\n",
    "print(len(clean_cars[clean_cars.fuel.isnull()]))\n",
    "print(len(clean_cars[clean_cars.odometer.isnull()]))\n",
    "print(len(clean_cars[clean_cars.title_status.isnull()]))\n",
    "print(len(clean_cars[clean_cars.transmission.isnull()]))\n",
    "print(len(clean_cars[clean_cars.drive.isnull()]))\n",
    "print(len(clean_cars[clean_cars.size.isnull()]))\n",
    "print(len(clean_cars[clean_cars.type.isnull()]))\n",
    "print(len(clean_cars[clean_cars.paint_color.isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf51a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_nulls = clean_cars[~clean_cars.year.isnull()]\n",
    "no_nulls.year = no_nulls.year.astype(int)\n",
    "no_nulls.odometer = no_nulls.odometer.fillna(0)\n",
    "no_nulls.odometer = no_nulls.odometer.astype(int)\n",
    "no_nulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a025b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_useless = no_nulls.copy()\n",
    "values_to_replace = remove_useless.model.value_counts()[remove_useless.model.value_counts() < 1000].index\n",
    "remove_useless.loc[remove_useless.model.isin(values_to_replace), 'model'] = np.nan\n",
    "remove_useless = remove_useless.rename(columns={'size': 'size1'})\n",
    "remove_useless.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer_dummies = pd.get_dummies(remove_useless.manufacturer, drop_first=True)\n",
    "model_dummies = pd.get_dummies(remove_useless.model, drop_first=True)\n",
    "condition_dummies = pd.get_dummies(remove_useless.condition, drop_first=True)\n",
    "cylinders_dummies = pd.get_dummies(remove_useless.cylinders, drop_first=True)\n",
    "fuel_dummies = pd.get_dummies(remove_useless.fuel, drop_first=True)\n",
    "title_status_dummies = pd.get_dummies(remove_useless.title_status, drop_first=True)\n",
    "transmission_dummies = pd.get_dummies(remove_useless.transmission, drop_first=True)\n",
    "drive_dummies = pd.get_dummies(remove_useless.drive, drop_first=True)\n",
    "size1_dummies = pd.get_dummies(remove_useless.size1, drop_first=True)\n",
    "type_dummies = pd.get_dummies(remove_useless.type, drop_first=True)\n",
    "paint_color_dummies = pd.get_dummies(remove_useless.paint_color, drop_first=True)\n",
    "final_df = pd.concat([remove_useless, manufacturer_dummies, model_dummies, condition_dummies, cylinders_dummies,\n",
    "                     fuel_dummies, title_status_dummies, transmission_dummies, drive_dummies, size1_dummies,\n",
    "                     type_dummies, paint_color_dummies], axis=\"columns\")\n",
    "final_df = final_df.drop([\"manufacturer\", \"model\", \"condition\", \"cylinders\", \"fuel\", \"title_status\", \"transmission\", \n",
    "                         \"drive\", \"size1\", \"type\", \"paint_color\"], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"please_god.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508311f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  },\n",
    "  'suppor_vector_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', SVR())\n",
    "    ],\n",
    "    'params': {\n",
    "        'regressor__kernel': ['linear'],\n",
    "        'regressor__C': [10],\n",
    "        'regressor__epsilon': [0.1],\n",
    "        'regressor__gamma': ['auto']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41995da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel=\"linear\", C=10, epsilon=0.1, gamma=\"auto\")\n",
    "grid_search = GridSearchCV(model, [], cv=5, return_train_score=False, verbose = 4)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45755d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Airbnb-US-2023.csv\")\n",
    "# Give some columns clearer/shorter names\n",
    "df = df.rename(columns={\n",
    "    \"price\": \"price_per_night\",\n",
    "    \"number_of_reviews\": \"nr_of_reviews\",\n",
    "    \"last_review\": \"last_review_date\",\n",
    "    \"calculated_host_listings_count\": \"nr_of_listings_by_host\",\n",
    "    \"availability_365\": \"days_per_year_available\",\n",
    "    \"number_of_reviews_ltm\": \"nr_of_reviews_last_twelve_months\"\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34a3e3",
   "metadata": {},
   "source": [
    "## Visualizing the locations\n",
    "### Use the output of the function below with google my maps to plot a number of locations unto a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_locations_to_google_my_maps(dataframe, number_of_locations = 10000):\n",
    "    \"\"\"Extracts latitude, longitude, name, and room_type from the dataframe provided.\n",
    "    The locations are reduced until their number is close to number_of_locations.\n",
    "    The result is grouped based on room_type, each group is exported to multiple csv files of max length = 2000.\n",
    "    These are to be imported on Google My Maps for plotting.\"\"\"\n",
    "    \n",
    "    dataframe = dataframe[[\"latitude\", \"longitude\", \"name\", \"room_type\"]]\n",
    "    \n",
    "    entire_apartment_df = dataframe[dataframe[\"room_type\"] == \"Entire home/apt\"].drop(\"room_type\", axis=1)\n",
    "    private_room_df = dataframe[dataframe[\"room_type\"] == \"Private room\"].drop(\"room_type\", axis=1)\n",
    "    shared_room_df = dataframe[dataframe[\"room_type\"] == \"Shared room\"].drop(\"room_type\", axis=1)\n",
    "    hotel_room_df = dataframe[dataframe[\"room_type\"] == \"Hotel room\"].drop(\"room_type\", axis=1)\n",
    "    \n",
    "    locations_to_eliminate = len(dataframe) - number_of_locations\n",
    "    entire_apartment_ratio = len(entire_apartment_df) / len(dataframe)\n",
    "    private_room_ratio = len(private_room_df) / len(dataframe)\n",
    "    shared_room_ratio = len(shared_room_df) / len(dataframe)\n",
    "    hotel_room_ratio = len(hotel_room_df) / len(dataframe)\n",
    "    \n",
    "    entire_apartment_locations_to_delete = int(entire_apartment_ratio * locations_to_eliminate)\n",
    "    private_room_locations_to_delete = int(private_room_ratio * locations_to_eliminate)\n",
    "    shared_room_locations_to_delete = int(shared_room_ratio * locations_to_eliminate)\n",
    "    hotel_room_locations_to_delete = int(hotel_room_ratio * locations_to_eliminate)\n",
    "    \n",
    "    entire_apartment_locations_to_keep = len(entire_apartment_df) - entire_apartment_locations_to_delete\n",
    "    private_room_locations_to_keep = len(private_room_df) - private_room_locations_to_delete\n",
    "    shared_room_locations_to_keep = len(shared_room_df) - shared_room_locations_to_delete\n",
    "    hotel_room_locations_to_keep = len(hotel_room_df) - hotel_room_locations_to_delete\n",
    "    \n",
    "    entire_apartment_df_shortened = entire_apartment_df.sample(entire_apartment_locations_to_keep)\n",
    "    private_room_df_shortened = private_room_df.sample(private_room_locations_to_keep)\n",
    "    shared_room_df_shortened = shared_room_df.sample(shared_room_locations_to_keep)\n",
    "    hotel_room_df_shortened = hotel_room_df.sample(hotel_room_locations_to_keep)\n",
    "    \n",
    "    entire_apartment_dfs = np.array_split(entire_apartment_df_shortened, len(entire_apartment_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(entire_apartment_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/apartment{i}.csv\")\n",
    "    \n",
    "    private_room_dfs = np.array_split(private_room_df_shortened, len(private_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(private_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/private{i}.csv\")\n",
    "    \n",
    "    shared_room_dfs = np.array_split(shared_room_df_shortened, len(shared_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(shared_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/shared{i}.csv\")\n",
    "    \n",
    "    hotel_room_dfs = np.array_split(hotel_room_df_shortened, len(hotel_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(hotel_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/hotel{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c63736",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(df)}\")\n",
    "relevant_df = df.drop([\"id\"], axis=1)\n",
    "relevant_df = relevant_df.drop_duplicates()\n",
    "# We drop neighbourhood_group because we already have the neighbourhood column which provides more information\n",
    "relevant_df = relevant_df.drop([\"host_id\", \"host_name\", \"latitude\", \"longitude\", \"neighbourhood_group\"], axis=1)\n",
    "print(f\"Length after removing duplicates: {len(relevant_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ad61b",
   "metadata": {},
   "source": [
    "### Dealing with missing values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ae120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "relevant_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56e0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Length before removing irrelevant columns: {len(relevant_df)}\")\n",
    "non_null_df = relevant_df.copy()\n",
    "non_null_df = non_null_df.dropna(subset=[\"name\"])\n",
    "non_null_df.last_review_date = non_null_df.last_review_date.fillna(0)\n",
    "non_null_df.reviews_per_month = non_null_df.reviews_per_month.fillna(0)\n",
    "non_null_df.loc[non_null_df.last_review_date != 0, \"last_review_date\"] = pd.to_datetime(non_null_df[non_null_df.last_review_date != 0].last_review_date).apply(lambda x: int(x.timestamp()))\n",
    "print(f\"Length after removing irrelevant columns: {len(non_null_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c4e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = non_null_df.drop([\"name\", \"neighbourhood\", \"room_type\", \"city\"], axis=1).columns\n",
    "\n",
    "rows_of_subplots = int(len(columns_used_for_checking_outliers) / 2)\n",
    "columns_of_subplots = 2\n",
    "\n",
    "fig, axes = plt.subplots(rows_of_subplots, columns_of_subplots, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(non_null_df[column_name], bins=20, rwidth=0.8)\n",
    "    \n",
    "    mean = non_null_df[column_name].mean()\n",
    "    standard_deviation = non_null_df[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (2 * standard_deviation)\n",
    "    upper_bound = mean + (2 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    # most subplots need to be scaled logarithmically because there is one range of values that is much more frequent\n",
    "    # than any other, thus the subplot won't give too much information unless it is scaled\n",
    "    if column_name != \"days_per_year_available\":\n",
    "        ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522fa64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# last_review_date and days_per_year_available will be excluded from outlier removal because they don't have any outliers\n",
    "\n",
    "columns_used_for_removing_outliers = non_null_df.drop([\"name\", \"neighbourhood\", \"room_type\", \"city\", \"last_review_date\", \"days_per_year_available\"], axis=1).columns\n",
    "\n",
    "print(f\"Length before removing outliers: {len(non_null_df)}\\n\")\n",
    "\n",
    "no_outliers_df = non_null_df.copy()\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers_df[column_name].mean()\n",
    "    standard_deviation = no_outliers_df[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (2 * standard_deviation)\n",
    "    upper_bound = mean + (2 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers_df[column_name] < lower_bound) | (no_outliers_df[column_name] > upper_bound)).sum() / len(no_outliers_df)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers_df = no_outliers_df[(lower_bound <= no_outliers_df[column_name]) & (no_outliers_df[column_name] <= upper_bound)]\n",
    "\n",
    "print(f\"\\nLength after removing outliers: {len(no_outliers_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27147bdd",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0721cf2",
   "metadata": {},
   "source": [
    "### Extracting meaningful words from name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d81f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_meaningful_words_df = no_outliers_df.copy()\n",
    "name_meaningful_words_df.name = name_meaningful_words_df.name.str.lower()\n",
    "# Extract number of bedrooms in a separate column\n",
    "name_meaningful_words_df[\"nr_of_bedrooms\"] = name_meaningful_words_df.name.str.extract('(\\d+)\\s*-?\\+?(?:br|bd|bed)')\n",
    "name_meaningful_words_df[\"has_nr_of_bedrooms\"] = ~name_meaningful_words_df[\"nr_of_bedrooms\"].isnull()\n",
    "name_meaningful_words_df[\"nr_of_bedrooms\"] = name_meaningful_words_df[\"nr_of_bedrooms\"].fillna(0)\n",
    "\n",
    "def most_frequent_words_in_name_column(dataframe):\n",
    "    # filter anything that is not helpful, or any words that are related to location\n",
    "    filtered_strings = {\"in\", \"private\", \"to\", \"bedroom\", \"the\", \"room\", \"home\", \"with\", \"apartment\", \"near\", \"and\", \"house\", \"of\", \n",
    "    \"a\", \"from\", \"bed\", \"apt\", \"bath\", \"close\", \"view\", \"suite\", \"on\", \"walk\", \"location\", \"for\", \"east\", \"by\", \"at\", \"hollywood\", \n",
    "    \"city\", \"one\", \"austin\", \"unit\", \"br\", \"nr\", \"west\", \"s\", \"d\", \"dt\", \"no\", \"bd\", \"b\", \"la\", \"full\", \"brooklyn\", \"san\", \"brand\", \n",
    "    \"hill\", \"steps\", \"minutes\", \"bathroom\", \"strip\", \"vegas\", \"manhattan\", \"prime\", \"las\", \"hills\", \"south\", \"mins\", \"hot\", \"guest\", \n",
    "    \"two\", \"free\", \"min\", \"hotel\", \"nyc\", \"place\", \"w\", \"floor\", \"stay\", \"nashville\", \"space\", \"shared\", \"away\", \"north\", \"entire\", \n",
    "    \"beds\", \"sleeps\", \"bay\", \"williamsburg\", \"side\", \"living\", \"kitchen\", \"rental\", \"square\", \"district\", \"located\", \"street\", \"area\", \"your\", \n",
    "    \"upper\", \"neighborhood\", \"pet\", \"st\", \"bdrm\", \"venice\", \"los\", \"style\", \"santa\", \"bedrooms\", \"blocks\", \"heights\", \"diego\", \n",
    "    \"block\", \"next\", \"miles\", \"capitol\", \"long\", \"centre\", \"dtla\", \"top\", \"broadway\", \"seattle\", \"mission\", \"all\"}\n",
    "\n",
    "    word_frequencies = dataframe.name.str.split('\\\\W+', expand=True).stack().value_counts()\n",
    "    word_frequencies = word_frequencies[(word_frequencies >= 1000) & word_frequencies.index.str.isalpha()]\n",
    "    word_frequencies = word_frequencies[~word_frequencies.index.isin(filtered_strings)]\n",
    "    \n",
    "    return word_frequencies\n",
    "\n",
    "most_frequent_words_in_name_column(name_meaningful_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8b2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequent words have to be manually extracted because similar words were grouped together to reduce number of columns\n",
    "# And some were removed due to not being useful\n",
    "meaningful_words_with_column_name = [\n",
    "    [\"peaceful\", [\"quiet\", \"oasis\", \"retreat\", \"getaway\", \"peaceful\"]],\n",
    "    [\"luxurios\", [\"luxury\", \"luxurious\", \"resort\", \"deluxe\"]],\n",
    "    [\"lovely\", [\"lovely\", \"charming\", \"cute\", \"nice\"]],\n",
    "    [\"spacious\", [\"spacious\", \"large\", \"huge\", \"big\"]],\n",
    "    [\"comfortable\", [\"cozy\", \"comfy\", \"comfortable\"]],\n",
    "    [\"central\", [\"downton\", \"heart\", \"central\", \"midtown\"]],\n",
    "    [\"amazing\", [\"great\", \"amazing\", \"best\", \"perfect\", \"paradise\"]],\n",
    "    [\"beautiful\", [\"beautiful\", \"gorgeous\", \"stunning\"]],\n",
    "    [\"bright\", [\"sunny\", \"bright\", \"cheerful\"]],\n",
    "    [\"renovated\", [\"renovated\", \"remodeled\", \"newly\"]],\n",
    "    [\"backyard\", [\"garden\", \"backyard\", \"yard\"]],\n",
    "    [\"beach\", [\"beach\", \"ocean\"]],\n",
    "    [\"pool\", [\"pool\", \"tub\"]],\n",
    "    [\"patio\", [\"deck\", \"patio\"]],\n",
    "    [\"stylish\", [\"stylish\", \"chic\"]],\n",
    "    [\"convenient\", [\"convenient\", \"everything\"]],\n",
    "    [\"townhouse\", [\"townhouse\", \"townhome\"]],\n",
    "    [\"urban\", [\"urban\", \"town\",]],\n",
    "    [\"modern\", [\"modern\", \"new\",]],\n",
    "    [\"village\", [\"village\"]],\n",
    "    [\"cottage\", [\"cottage\"]],\n",
    "    [\"studio\", [\"studio\"]],\n",
    "    [\"condo\", [\"condo\"]],\n",
    "    [\"bungalow\", [\"bungalow\"]],\n",
    "    [\"villa\", [\"villa\"]],\n",
    "    [\"penthouse\", [\"penthouse\"]],\n",
    "    [\"duplex\", [\"duplex\"]],\n",
    "    [\"loft\", [\"loft\"]],\n",
    "    [\"king\", [\"king\"]],\n",
    "    [\"queen\", [\"queen\"]],\n",
    "    [\"master\", [\"master\"]],\n",
    "    [\"gym\", [\"gym\"]],\n",
    "    [\"entrance\", [\"entrance\"]],\n",
    "    [\"balcony\", [\"balcony\"]],\n",
    "    [\"rooftop\", [\"rooftop\"]],\n",
    "    [\"mid\", [\"mid\"]],\n",
    "    [\"victorian\", [\"victorian\"]],\n",
    "    [\"historic\", [\"historic\"]],\n",
    "    [\"gem\", [\"gem\"]],\n",
    "    [\"clean\", [\"clean\"]],\n",
    "    [\"furnished\", [\"furnished\"]],\n",
    "    [\"family\", [\"family\"]],\n",
    "    [\"friendly\", [\"friendly\"]],\n",
    "    [\"views\", [\"views\"]],\n",
    "    [\"valley\", [\"valley\"]],\n",
    "    [\"park\", [\"park\"]],\n",
    "    [\"lake\", [\"lake\"]],\n",
    "    [\"waterfront\", [\"waterfront\"]],\n",
    "    [\"vacation\", [\"vacation\"]],\n",
    "    [\"spa\", [\"spa\"]],\n",
    "    [\"heated\", [\"heated\"]],\n",
    "    [\"airport\", [\"airport\"]],\n",
    "    [\"parking\", [\"parking\"]],\n",
    "    [\"wifi\", [\"wifi\"]]\n",
    "]\n",
    "\n",
    "# for each meaningful word column (column_name) create a new column in the dataframe with its name (e.g. luxurious)\n",
    "# split each row's name with delimiters being whitespace or special characters\n",
    "# if any word in the split name is inside a list of words (values_to_search)\n",
    "# set the value for the current column to True, otherwise set to False\n",
    "for column_name, values_to_search in meaningful_words_with_column_name:\n",
    "    name_meaningful_words_df[column_name] = name_meaningful_words_df.name.str.split('\\\\W+').apply(lambda cur_word: any(value_to_search in cur_word for value_to_search in values_to_search))\n",
    "\n",
    "name_meaningful_words_df = name_meaningful_words_df.drop(\"name\", axis=1)\n",
    "name_meaningful_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a21f20",
   "metadata": {},
   "source": [
    "### Checking if city column needs any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd439eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There aren't too many cities, so the values will remain unmodified until they are one-hot encoded\n",
    "name_meaningful_words_df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040321a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_frequency_values_removed_df = name_meaningful_words_df.copy()\n",
    "values_to_replace = reduced_frequency_values_removed_df.city.value_counts()[reduced_frequency_values_removed_df.city.value_counts() < 5000].index\n",
    "reduced_frequency_values_removed_df.loc[reduced_frequency_values_removed_df.city.isin(values_to_replace), 'city'] = 'Other'\n",
    "reduced_frequency_values_removed_df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f318483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are many values with a frequency of one, everything with a frequency < 1000 will be converted to 'Other'\n",
    "name_meaningful_words_df.neighbourhood.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_replace = reduced_frequency_values_removed_df.neighbourhood.value_counts()[reduced_frequency_values_removed_df.neighbourhood.value_counts() < 1000].index\n",
    "reduced_frequency_values_removed_df.loc[reduced_frequency_values_removed_df.neighbourhood.isin(values_to_replace), 'neighbourhood'] = 'Other'\n",
    "reduced_frequency_values_removed_df.neighbourhood.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e6299",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5328cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding the room_type, city and neighbourhood columns\n",
    "room_type_dummies = pd.get_dummies(reduced_frequency_values_removed_df.room_type, drop_first=True)\n",
    "city_dummies = pd.get_dummies(reduced_frequency_values_removed_df.city, drop_first=True)\n",
    "neighbourhood_dummies = pd.get_dummies(reduced_frequency_values_removed_df.neighbourhood, drop_first=True)\n",
    "final_df = pd.concat([reduced_frequency_values_removed_df, room_type_dummies, neighbourhood_dummies, city_dummies], axis=\"columns\")\n",
    "final_df = final_df.drop([\"neighbourhood\", \"room_type\", \"city\"], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c13e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe374e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6905a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['price_per_night'] = pd.cut(final_df['price_per_night'], bins=[-1, 90, 150, 257, float('inf')],\n",
    "                           labels=[0, 1, 2, 3])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92765de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"df_full_size.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446daa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df[final_df.has_nr_of_bedrooms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762e9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32acf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"loosing_my_neurons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df.price_per_night1.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956442e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LogisticRegression(max_iter=10000))\n",
    "    ],\n",
    "    'params': {}\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc6634",
   "metadata": {},
   "source": [
    "## Testing ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  },\n",
    "  'suppor_vector_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', SVR())\n",
    "    ],\n",
    "    'params': {\n",
    "        'regressor__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'regressor__C': [0.1, 10],\n",
    "        'regressor__epsilon': [0.1, 0.001],\n",
    "        'regressor__gamma': ['scale', 'auto']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23a795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
