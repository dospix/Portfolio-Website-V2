{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./used_cars.csv\")\n",
    "cars = cars.rename(columns={\n",
    "    \"year\": \"entry_year\",\n",
    "    \"title_status\": \"vehicle_status\",\n",
    "    \"size\": \"vehicle_size\",\n",
    "    \"type\": \"vehicle_type\"\n",
    "})\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(cars)}\")\n",
    "\n",
    "clean_cars = cars.drop([\"id\", \"region\", \"VIN\", \"county\", \"lat\", \"long\", \"posting_date\"], axis=1)\n",
    "# These columns are removed because they were found to have no correlation in a future heatmap\n",
    "clean_cars = clean_cars.drop([\"paint_color\", \"state\"], axis=1)\n",
    "clean_cars.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "print(f\"Length after removing duplicates: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab5b1b",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_null_values_count_per_column(dataframe):\n",
    "    end_output = \"\"\n",
    "    for column in dataframe.columns:\n",
    "        end_output += f\"nulls in {column}: {len(dataframe[dataframe[column].isnull()])},\\n\"\n",
    "    end_output = end_output.rstrip(\",\\n\")\n",
    "    print(end_output)\n",
    "\n",
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(clean_cars)}\")\n",
    "\n",
    "# Car model is essential for predicting price, thus null values are dropped\n",
    "clean_cars = clean_cars.dropna(subset=\"model\")\n",
    "\n",
    "# year and odometer nulls are difficult to fill, since there are few of them they will be dropped\n",
    "clean_cars = clean_cars.dropna(subset=[\"entry_year\", \"odometer\"])\n",
    "\n",
    "# for columns with few null values, merge them in the most common category\n",
    "# otherwise place them in their own \"unknown\" group\n",
    "clean_cars.manufacturer = clean_cars.manufacturer.fillna(\"unknown\")\n",
    "clean_cars.condition = clean_cars.condition.fillna(\"unknown\")\n",
    "clean_cars.cylinders = clean_cars.cylinders.fillna(\"unknown\")\n",
    "clean_cars.fuel = clean_cars.fuel.fillna(\"gas\")\n",
    "clean_cars.vehicle_status = clean_cars.vehicle_status.fillna(\"clean\")\n",
    "clean_cars.transmission = clean_cars.transmission.fillna(\"automatic\")\n",
    "clean_cars.drive = clean_cars.drive.fillna(\"unknown\")\n",
    "clean_cars.vehicle_size = clean_cars.vehicle_size.fillna(\"unknown\")\n",
    "clean_cars.vehicle_type = clean_cars.vehicle_type.fillna(\"unknown\")\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ea78c",
   "metadata": {},
   "source": [
    "### Removing all rows that describe the same car but different price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec08314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(clean_cars)}\")\n",
    "\n",
    "rows_to_remove = clean_cars[clean_cars.drop(\"price\", axis=1).duplicated(keep=False)].index\n",
    "clean_cars = clean_cars.drop(rows_to_remove, axis=0)\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc0973",
   "metadata": {},
   "source": [
    "### Changing string columns to numerical columns where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cars.condition = clean_cars.condition.map({\n",
    "    \"unknown\": -1,\n",
    "    \"salvage\": 0,\n",
    "    \"fair\": 1,\n",
    "    \"good\": 2,\n",
    "    \"excellent\": 3,\n",
    "    \"like new\": 4,\n",
    "    \"new\": 5\n",
    "})\n",
    "clean_cars.cylinders = clean_cars.cylinders.map({\n",
    "    \"unknown\": -1,\n",
    "    \"other\": 0,\n",
    "    \"3 cylinders\": 3,\n",
    "    \"4 cylinders\": 4,\n",
    "    \"5 cylinders\": 5,\n",
    "    \"6 cylinders\": 6,\n",
    "    \"8 cylinders\": 8,\n",
    "    \"10 cylinders\": 10,\n",
    "    \"12 cylinders\": 12\n",
    "})\n",
    "clean_cars.vehicle_size = clean_cars.vehicle_size.map({\n",
    "    \"unknown\": -1,\n",
    "    \"sub-compact\": 0,\n",
    "    \"compact\": 1,\n",
    "    \"mid-size\": 2,\n",
    "    \"full-size\": 3\n",
    "})\n",
    "\n",
    "clean_cars.price = clean_cars.price.astype(int)\n",
    "clean_cars.entry_year = clean_cars.entry_year.astype(int)\n",
    "clean_cars.odometer = clean_cars.odometer.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff062",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a07e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all prices under 1M$ because big prices mess with the histogram below\n",
    "no_outliers = clean_cars.copy()\n",
    "no_outliers.price = no_outliers.price[no_outliers.price < 1000000]\n",
    "no_outliers.price = no_outliers.price[no_outliers.price >= 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d08c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=75, rwidth=0.8)\n",
    "    \n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    if column_name != \"entry_year\":\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(f\"Distribution of {column_name} (logarithmic scale)\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98025b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_used_for_removing_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers[column_name] < lower_bound) | (no_outliers[column_name] > upper_bound)).sum() / len(no_outliers)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers = no_outliers[(lower_bound <= no_outliers[column_name]) & (no_outliers[column_name] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=25, rwidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db16fa5",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f7fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_counts = no_outliers.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 25].index\n",
    "no_outliers = no_outliers[no_outliers.model.isin(values_to_keep)]\n",
    "\n",
    "no_outliers.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db367a",
   "metadata": {},
   "source": [
    "### Erase price outliers for each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_model_groups = no_outliers.groupby('model')\n",
    "\n",
    "def remove_outliers(group):\n",
    "    price_mean = group['price'].mean()\n",
    "    price_std = group['price'].std()\n",
    "\n",
    "    lower_bound = price_mean - (2 * price_std)\n",
    "    upper_bound = price_mean + (2 * price_std)\n",
    "\n",
    "    lower_outliers_mask = group['price'] >= lower_bound\n",
    "    upper_outliers_mask = group['price'] <= upper_bound\n",
    "\n",
    "    return group[lower_outliers_mask & upper_outliers_mask]\n",
    "\n",
    "no_outliers = car_model_groups.apply(remove_outliers)\n",
    "no_outliers.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee16dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Length before removing outliers: {len(clean_cars)}\\n\")\n",
    "print(f\"Length after removing outliers: {len(no_outliers)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e45cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model_name in enumerate(final1_df.model.unique()):\n",
    "    print(index + 1, len(final1_df.model.unique()))\n",
    "    curr_cars = final1_df[final1_df.model == model_name].sort_values(by='price', ascending=False)\n",
    "    to_be_deleted = set()\n",
    "    for i, row in curr_cars.iterrows():\n",
    "        if i in to_be_deleted:\n",
    "            continue\n",
    "        better_cars = curr_cars[\n",
    "            (curr_cars['manufacturer'] == row['manufacturer']) &\n",
    "            (curr_cars['entry_year'] >= row['entry_year']) &\n",
    "            (curr_cars['condition'] >= row['condition']) &\n",
    "            (curr_cars['condition_unknown'] == row['condition_unknown']) &\n",
    "            (curr_cars['cylinders'] >= row['cylinders']) &\n",
    "            (curr_cars['cylinders_unknown'] == row['cylinders_unknown']) &\n",
    "            (curr_cars['fuel'] == row['fuel']) &\n",
    "            (curr_cars['odometer'] <= row['odometer']) &\n",
    "            (curr_cars['vehicle_status'] == row['vehicle_status']) &\n",
    "            (curr_cars['transmission'] == row['transmission']) &\n",
    "            (curr_cars['drive'] == row['drive']) &\n",
    "            (curr_cars['vehicle_size'] >= row['vehicle_size']) &\n",
    "            (curr_cars['vehicle_size_unknown'] == row['vehicle_size_unknown']) &\n",
    "            (curr_cars['vehicle_type'] == row['vehicle_type']) &\n",
    "            (curr_cars.index != i)\n",
    "        ]\n",
    "        if len(better_cars[better_cars['price'] <= row['price']]):\n",
    "            to_be_deleted.update(better_cars[better_cars['price'] <= row['price']].index)\n",
    "            print(row, better_cars[better_cars['price'] <= row['price']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27555aac",
   "metadata": {},
   "source": [
    "### Eliminating better cars that are cheaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = no_outliers.copy()\n",
    "\n",
    "final_df[\"condition_unknown\"] = np.where(final_df[\"condition\"] == -1, 1, 0)\n",
    "final_df[\"cylinders_unknown\"] = np.where(final_df[\"cylinders\"] == -1, 1, 0)\n",
    "final_df[\"vehicle_size_unknown\"] = np.where(final_df[\"vehicle_size\"] == -1, 1, 0)\n",
    "\n",
    "for model_index, model_name in tqdm(enumerate(final_df.model.unique()), total=len(final_df.model.unique())):\n",
    "    curr_cars = final_df[final_df.model == model_name].sort_values(by='price', ascending=False)\n",
    "    to_be_deleted = set()\n",
    "    for car_index, car in curr_cars.iterrows():\n",
    "        if car_index in to_be_deleted:\n",
    "            continue\n",
    "        better_cheaper_cars = curr_cars[\n",
    "            (curr_cars.manufacturer == car.manufacturer) &\n",
    "            (curr_cars.entry_year >= car.entry_year) &\n",
    "            (curr_cars.condition >= car.condition) &\n",
    "            (curr_cars.condition_unknown == car.condition_unknown) &\n",
    "            (curr_cars.cylinders >= car.cylinders) &\n",
    "            (curr_cars.cylinders_unknown == car.cylinders_unknown) &\n",
    "            (curr_cars.fuel == car.fuel) &\n",
    "            (curr_cars.odometer <= car.odometer) &\n",
    "            (curr_cars.vehicle_status == car.vehicle_status) &\n",
    "            (curr_cars.transmission == car.transmission) &\n",
    "            (curr_cars.drive == car.drive) &\n",
    "            (curr_cars.vehicle_size >= car.vehicle_size) &\n",
    "            (curr_cars.vehicle_size_unknown == car.vehicle_size_unknown) &\n",
    "            (curr_cars.vehicle_type == car.vehicle_type) &\n",
    "            (curr_cars.price <= car.price) &\n",
    "            (curr_cars.index != car_index)\n",
    "        ]\n",
    "        \n",
    "        if len(better_cheaper_cars):\n",
    "            to_be_deleted.update(better_cheaper_cars.index)\n",
    "    final_df = final_df.drop(to_be_deleted)\n",
    "\n",
    "final_df = final_df.drop([\"condition_unknown\", \"cylinders_unknown\", \"vehicle_size_unknown\"], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "blabla = final_df.copy()\n",
    "\n",
    "# Label encode the string columns\n",
    "label_encoder = LabelEncoder()\n",
    "string_columns = ['manufacturer', 'model', \"condition\", \"cylinders\", 'fuel', 'vehicle_status', 'transmission', 'drive', \"vehicle_size\", 'vehicle_type']\n",
    "for col in string_columns:\n",
    "    blabla[col] = label_encoder.fit_transform(blabla[col])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = blabla.corr()\n",
    "\n",
    "# Create a correlation heatmap using matplotlib\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "# Annotate the heatmap with correlation values\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(correlation_matrix.shape[1]):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='white', fontsize=8)\n",
    "\n",
    "# Set ticks and labels\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.index)), correlation_matrix.index)\n",
    "\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c11d33",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counts = final_df.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 25].index\n",
    "final_df = final_df[final_df.model.isin(values_to_keep)]\n",
    "\n",
    "final_df.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c7a2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\"manufacturer\", \"model\", \"fuel\", \"vehicle_status\", \"transmission\", \"drive\", \"vehicle_type\"]\n",
    "\n",
    "final_df = pd.get_dummies(final_df, columns=columns_to_encode, prefix=columns_to_encode, drop_first=True)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508311f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "models = { \n",
    "    \"linear_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", linear_model.LinearRegression())\n",
    "        ],\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"knn_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_neighbors\": [3, 5, 7],\n",
    "            \"regressor__weights\": [\"uniform\", \"distance\"],\n",
    "            \"regressor__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "            \"regressor__leaf_size\": [20, 30, 40],\n",
    "            \"regressor__p\": [1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"suppor_vector_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", SVR())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"regressor__C\": [0.1, 1, 10],\n",
    "            \"regressor__epsilon\": [0.1, 0.01],\n",
    "            \"regressor__gamma\": [\"scale\", \"auto\", 0.1, 1],\n",
    "            \"regressor__degree\": [2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"random_forest_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", RandomForestRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_estimators\": [50, 100, 200],\n",
    "            \"regressor__max_depth\": [None, 5, 10],\n",
    "            \"regressor__min_samples_split\": [2, 5],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 4],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \"gradient_boosting_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", GradientBoostingRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__learning_rate\": [0.1, 0.01, 0.001],\n",
    "            \"regressor__n_estimators\": [50, 100, 200],\n",
    "            \"regressor__max_depth\": [None, 5, 10],\n",
    "            \"regressor__min_samples_split\": [2, 5],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 4],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    score_results = grid_search.cv_results_['mean_test_score']\n",
    "    params_results = grid_search.cv_results_['params']\n",
    "    \n",
    "    for score, params in zip(score_results, params_results):\n",
    "        scores.append({\n",
    "            'model': model_name,\n",
    "            'score': score,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'score', 'params'])\n",
    "scores_df = scores_df.sort_values('score', ascending=False)\n",
    "scores_df.reset_index(drop=True, inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b1777",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.loc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, max_depth=None, max_features=\"sqrt\", min_samples_leaf=2, min_samples_split=2, n_estimators=200)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522022a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers[no_outliers.price == 1500].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d946fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame({'residuals': residuals})  # Create a DataFrame with residuals\n",
    "df_inputs = X_test.reset_index(drop=True)  # Reset the index of X_test DataFrame\n",
    "\n",
    "# Find rows with residuals greater than 10000\n",
    "outliers_dataframe = df_residuals[abs(df_residuals['residuals']) > 1500]\n",
    "\n",
    "# Get the corresponding inputs for outliers\n",
    "outliers_inputs = outliers_dataframe.loc[outliers_dataframe.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[38903]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5193b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([final_df.loc[185851].drop(\"price\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb427fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers.loc[185851]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90392f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers[no_outliers.model == \"q7\"].price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers[no_outliers.model == \"q7\"].price.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52755343",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers[no_outliers.model == \"q7\"].price.mean() - no_outliers[no_outliers.model == \"q7\"].price.std() - no_outliers[no_outliers.model == \"q7\"].price.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
