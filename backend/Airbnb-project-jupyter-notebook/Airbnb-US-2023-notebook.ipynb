{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eca7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./vehicles.csv\")\n",
    "useless_removed = cars.drop([\"url\", \"region_url\", \"image_url\", \"description\"], axis=1)\n",
    "useless_removed.to_csv(\"./used_cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./vehicles.csv\")\n",
    "cars = cars.rename(columns={\n",
    "    \"year\": \"entry_year\",\n",
    "    \"title_status\": \"vehicle_status\",\n",
    "    \"size\": \"vehicle_size\",\n",
    "    \"type\": \"vehicle_type\"\n",
    "})\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(cars)}\")\n",
    "\n",
    "clean_cars = cars.drop([\"id\", \"url\", \"region\", \"region_url\", \"VIN\", \"image_url\", \"description\", \"county\", \"state\", \"lat\", \"long\", \"posting_date\"], axis=1)\n",
    "clean_cars = clean_cars.drop_duplicates()\n",
    "\n",
    "print(f\"Length after removing duplicates: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab5b1b",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_null_values_count_per_column(dataframe):\n",
    "    end_output = \"\"\n",
    "    for column in dataframe.columns:\n",
    "        end_output += f\"nulls in {column}: {len(dataframe[dataframe[column].isnull()])},\\n\"\n",
    "    end_output = end_output.rstrip(\",\\n\")\n",
    "    print(end_output)\n",
    "\n",
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(clean_cars)}\")\n",
    "\n",
    "# Car model is essential for predicting price, thus null values are dropped\n",
    "no_nulls = clean_cars.copy()\n",
    "no_nulls = no_nulls.dropna(subset=\"model\")\n",
    "\n",
    "# year and odometer nulls are difficult to fill, since there are few of them they will be dropped\n",
    "no_nulls = no_nulls.dropna(subset=[\"entry_year\", \"odometer\"])\n",
    "\n",
    "# for columns with few null values, merge them in the most common category\n",
    "# otherwise place them in their own \"unknown\" group\n",
    "no_nulls.manufacturer = no_nulls.manufacturer.fillna(\"unknown\")\n",
    "no_nulls.condition = no_nulls.condition.fillna(\"unknown\")\n",
    "no_nulls.cylinders = no_nulls.cylinders.fillna(\"unknown\")\n",
    "no_nulls.fuel = no_nulls.fuel.fillna(\"gas\")\n",
    "no_nulls.vehicle_status = no_nulls.vehicle_status.fillna(\"clean\")\n",
    "no_nulls.transmission = no_nulls.transmission.fillna(\"automatic\")\n",
    "no_nulls.drive = no_nulls.drive.fillna(\"unknown\")\n",
    "no_nulls.vehicle_size = no_nulls.vehicle_size.fillna(\"unknown\")\n",
    "no_nulls.vehicle_type = no_nulls.vehicle_type.fillna(\"unknown\")\n",
    "no_nulls.paint_color = no_nulls.paint_color.fillna(\"unknown\")\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(no_nulls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_null_values_count_per_column(no_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ea78c",
   "metadata": {},
   "source": [
    "### Removing all rows that describe the same car but different price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec08314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(no_nulls)}\")\n",
    "\n",
    "rows_to_remove = no_nulls[no_nulls.drop(\"price\", axis=1).duplicated(keep=False)].index\n",
    "no_nulls = no_nulls.drop(rows_to_remove, axis=0)\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(no_nulls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff062",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a07e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all prices under 1M$ because big prices mess with the histogram below\n",
    "no_outliers = no_nulls.copy()\n",
    "no_outliers.price = no_outliers.price[no_outliers.price < 1000000]\n",
    "no_outliers.price = no_outliers.price[no_outliers.price >= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d08c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=75, rwidth=0.8)\n",
    "    \n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    if column_name != \"entry_year\":\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(f\"Distribution of {column_name} (logarithmic scale)\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98025b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing outliers: {len(no_nulls)}\\n\")\n",
    "\n",
    "columns_used_for_removing_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers[column_name] < lower_bound) | (no_outliers[column_name] > upper_bound)).sum() / len(no_outliers)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers = no_outliers[(lower_bound <= no_outliers[column_name]) & (no_outliers[column_name] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db16fa5",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counts = no_outliers.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 10].index\n",
    "no_outliers = no_outliers[no_outliers.model.isin(values_to_keep)]\n",
    "\n",
    "no_outliers.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in no_outliers.model.unique():\n",
    "    prices = no_outliers[no_outliers.model == model_name].price\n",
    "    lower_bound = prices.mean() - (2 * prices.std())\n",
    "    upper_bound = prices.mean() + (2 * prices.std())\n",
    "    outliers = no_outliers[(no_outliers.model == model_name) & ((lower_bound > no_outliers.price) | (upper_bound < no_outliers.price))].index\n",
    "    no_outliers = no_outliers.drop(outliers)\n",
    "\n",
    "print(f\"\\nLength after removing outliers: {len(no_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=25, rwidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568b91f",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5266f0",
   "metadata": {},
   "source": [
    "### Changing string columns to numerical columns where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac47421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = no_outliers.copy()\n",
    "\n",
    "final_df.condition = final_df.condition.map({\n",
    "    \"unknown\": -1,\n",
    "    \"salvage\": 0,\n",
    "    \"fair\": 1,\n",
    "    \"good\": 2,\n",
    "    \"excellent\": 3,\n",
    "    \"like new\": 4,\n",
    "    \"new\": 5\n",
    "})\n",
    "final_df.cylinders = final_df.cylinders.map({\n",
    "    \"unknown\": -1,\n",
    "    \"other\": 0,\n",
    "    \"3 cylinders\": 3,\n",
    "    \"4 cylinders\": 4,\n",
    "    \"5 cylinders\": 5,\n",
    "    \"6 cylinders\": 6,\n",
    "    \"8 cylinders\": 8,\n",
    "    \"10 cylinders\": 10,\n",
    "    \"12 cylinders\": 12\n",
    "})\n",
    "final_df.vehicle_size = final_df.vehicle_size.map({\n",
    "    \"unknown\": -1,\n",
    "    \"sub-compact\": 0,\n",
    "    \"compact\": 1,\n",
    "    \"mid-size\": 2,\n",
    "    \"full-size\": 3\n",
    "})\n",
    "\n",
    "final_df[\"condition_unknown\"] = np.where(final_df[\"condition\"] == \"unknown\", 1, 0)\n",
    "final_df[\"cylinders_unknown\"] = np.where(final_df[\"cylinders\"] == \"unknown\", 1, 0)\n",
    "final_df[\"vehicle_size_unknown\"] = np.where(final_df[\"vehicle_size\"] == \"unknown\", 1, 0)\n",
    "\n",
    "final_df.price = final_df.price.astype(int)\n",
    "final_df.entry_year = final_df.entry_year.astype(int)\n",
    "final_df.odometer = final_df.odometer.astype(int)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c7a2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\"manufacturer\", \"model\", \"fuel\", \"vehicle_status\", \"transmission\", \"drive\", \"vehicle_type\", \"paint_color\"]\n",
    "\n",
    "final_df = pd.get_dummies(final_df, columns=columns_to_encode, prefix=columns_to_encode, drop_first=True)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508311f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "models = { \n",
    "    \"linear_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", linear_model.LinearRegression())\n",
    "        ],\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"knn_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_neighbors\": [3, 5, 7],\n",
    "            \"regressor__weights\": [\"uniform\", \"distance\"],\n",
    "            \"regressor__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "            \"regressor__leaf_size\": [20, 30, 40],\n",
    "            \"regressor__p\": [1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"suppor_vector_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", SVR())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"regressor__C\": [0.1, 1, 10],\n",
    "            \"regressor__epsilon\": [0.1, 0.01],\n",
    "            \"regressor__gamma\": [\"scale\", \"auto\", 0.1, 1],\n",
    "            \"regressor__degree\": [2, 3]\n",
    "        }\n",
    "    },\n",
    "    \"random_forest_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", RandomForestRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__n_estimators\": [50, 100, 200],\n",
    "            \"regressor__max_depth\": [None, 5, 10],\n",
    "            \"regressor__min_samples_split\": [2, 5],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 4],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    \"gradient_boosting_regression\": {\n",
    "        \"steps\": [\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"regressor\", GradientBoostingRegressor())\n",
    "        ],\n",
    "        \"params\": {\n",
    "            \"regressor__learning_rate\": [0.1, 0.01, 0.001],\n",
    "            \"regressor__n_estimators\": [50, 100, 200],\n",
    "            \"regressor__max_depth\": [None, 5, 10],\n",
    "            \"regressor__min_samples_split\": [2, 5],\n",
    "            \"regressor__min_samples_leaf\": [1, 2, 4],\n",
    "            \"regressor__max_features\": [1.0, 'sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    score_results = grid_search.cv_results_['mean_test_score']\n",
    "    params_results = grid_search.cv_results_['params']\n",
    "    \n",
    "    for score, params in zip(score_results, params_results):\n",
    "        scores.append({\n",
    "            'model': model_name,\n",
    "            'score': score,\n",
    "            'params': params\n",
    "        })\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'score', 'params'])\n",
    "scores_df = scores_df.sort_values('score', ascending=False)\n",
    "scores_df.reset_index(drop=True, inplace=True)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = GradientBoostingRegressor(learning_rate=0.1, max_depth=5, max_features=1.0, min_samples_leaf=4, min_samples_split = 5, n_estimators=100)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame({'residuals': residuals})  # Create a DataFrame with residuals\n",
    "df_inputs = X_test.reset_index(drop=True)  # Reset the index of X_test DataFrame\n",
    "\n",
    "# Find rows with residuals greater than 40000\n",
    "outliers_dataframe = df_residuals[abs(df_residuals['residuals']) > 20000]\n",
    "\n",
    "# Get the corresponding inputs for outliers\n",
    "outliers_inputs = outliers_dataframe.loc[outliers_dataframe.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[38903]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5193b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict([final_df.loc[38903].drop(\"price\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb427fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers.loc[38903]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
