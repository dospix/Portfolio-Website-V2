{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"./vehicles.csv\")\n",
    "cars = cars.rename(columns={\n",
    "    \"year\": \"entry_year\",\n",
    "    \"title_status\": \"vehicle_status\",\n",
    "    \"size\": \"vehicle_size\",\n",
    "    \"type\": \"vehicle_type\"\n",
    "})\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing duplicates: {len(cars)}\")\n",
    "\n",
    "clean_cars = cars.drop([\"id\", \"url\", \"region\", \"region_url\", \"VIN\", \"image_url\", \"description\", \"county\", \"state\", \"lat\", \"long\", \"posting_date\"], axis=1)\n",
    "clean_cars = clean_cars.drop_duplicates()\n",
    "\n",
    "print(f\"Length after removing duplicates: {len(clean_cars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab5b1b",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_null_values_count_per_column(dataframe):\n",
    "    end_output = \"\"\n",
    "    for column in dataframe.columns:\n",
    "        end_output += f\"nulls in {column}: {len(dataframe[dataframe[column].isnull()])},\\n\"\n",
    "    end_output = end_output.rstrip(\",\\n\")\n",
    "    print(end_output)\n",
    "\n",
    "print_null_values_count_per_column(clean_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(clean_cars)}\")\n",
    "\n",
    "# Car model is essential for predicting price, thus null values are dropped\n",
    "no_nulls = clean_cars.copy()\n",
    "no_nulls = no_nulls.dropna(subset=\"model\")\n",
    "\n",
    "# year and odometer nulls are difficult to fill, since there are few of them they will be dropped\n",
    "no_nulls = no_nulls.dropna(subset=[\"entry_year\", \"odometer\"])\n",
    "\n",
    "# for columns with few null values, merge them in the most common category\n",
    "# otherwise place them in their own \"unknown\" group\n",
    "no_nulls.manufacturer = no_nulls.manufacturer.fillna(\"unknown\")\n",
    "no_nulls.condition = no_nulls.condition.fillna(\"unknown\")\n",
    "no_nulls.cylinders = no_nulls.cylinders.fillna(\"unknown\")\n",
    "no_nulls.fuel = no_nulls.fuel.fillna(\"gas\")\n",
    "no_nulls.vehicle_status = no_nulls.vehicle_status.fillna(\"clean\")\n",
    "no_nulls.transmission = no_nulls.transmission.fillna(\"automatic\")\n",
    "no_nulls.drive = no_nulls.drive.fillna(\"unknown\")\n",
    "no_nulls.vehicle_size = no_nulls.vehicle_size.fillna(\"unknown\")\n",
    "no_nulls.vehicle_type = no_nulls.vehicle_type.fillna(\"unknown\")\n",
    "no_nulls.paint_color = no_nulls.paint_color.fillna(\"unknown\")\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(no_nulls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885e865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_null_values_count_per_column(no_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ea78c",
   "metadata": {},
   "source": [
    "### Removing all rows that describe the same car but different price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec08314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing same cars different price: {len(no_nulls)}\")\n",
    "\n",
    "rows_to_remove = no_nulls[no_nulls.drop(\"price\", axis=1).duplicated(keep=False)].index\n",
    "no_nulls = no_nulls.drop(rows_to_remove, axis=0)\n",
    "\n",
    "print(f\"Length after removing same cars different price: {len(no_nulls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f12585",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nulls[no_nulls.price > 1000000].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff062",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a07e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all prices under 1M$ because big prices mess with the histogram below\n",
    "no_outliers = no_nulls.copy()\n",
    "no_outliers.price = no_outliers.price[no_outliers.price < 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d08c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=75, rwidth=0.8)\n",
    "    \n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    if column_name != \"entry_year\":\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(f\"Distribution of {column_name} (logarithmic scale)\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98025b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing outliers: {len(no_nulls)}\\n\")\n",
    "\n",
    "columns_used_for_removing_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers[column_name].mean()\n",
    "    standard_deviation = no_outliers[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers[column_name] < lower_bound) | (no_outliers[column_name] > upper_bound)).sum() / len(no_outliers)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers = no_outliers[(lower_bound <= no_outliers[column_name]) & (no_outliers[column_name] <= upper_bound)]\n",
    "\n",
    "print(f\"\\nLength after removing outliers: {len(no_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_used_for_checking_outliers = [\"price\", \"entry_year\", \"odometer\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(no_outliers[column_name], bins=25, rwidth=0.8)\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568b91f",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5266f0",
   "metadata": {},
   "source": [
    "### Changing string columns to numerical columns where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac47421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = no_outliers.copy()\n",
    "\n",
    "final_df.condition = final_df.condition.map({\n",
    "    \"unknown\": -1,\n",
    "    \"salvage\": 0,\n",
    "    \"fair\": 1,\n",
    "    \"good\": 2,\n",
    "    \"excellent\": 3,\n",
    "    \"like new\": 4,\n",
    "    \"new\": 5\n",
    "})\n",
    "final_df.cylinders = final_df.cylinders.map({\n",
    "    \"unknown\": -1,\n",
    "    \"other\": 0,\n",
    "    \"3 cylinders\": 3,\n",
    "    \"4 cylinders\": 4,\n",
    "    \"5 cylinders\": 5,\n",
    "    \"6 cylinders\": 6,\n",
    "    \"8 cylinders\": 8,\n",
    "    \"10 cylinders\": 10,\n",
    "    \"12 cylinders\": 12\n",
    "})\n",
    "final_df.vehicle_size = final_df.vehicle_size.map({\n",
    "    \"unknown\": -1,\n",
    "    \"sub-compact\": 0,\n",
    "    \"compact\": 1,\n",
    "    \"mid-size\": 2,\n",
    "    \"full-size\": 3\n",
    "})\n",
    "\n",
    "final_df.price = final_df.price.astype(int)\n",
    "final_df.entry_year = final_df.entry_year.astype(int)\n",
    "final_df.odometer = final_df.odometer.astype(int)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d28b3a",
   "metadata": {},
   "source": [
    "### Erasing models that don't appear often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counts = final_df.model.value_counts()\n",
    "values_to_keep = model_counts[model_counts >= 10].index\n",
    "final_df = final_df[final_df.model.isin(values_to_keep)]\n",
    "\n",
    "final_df.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.vehicle_size[final_df.vehicle_size.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c142551",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cff0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.manufacturer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.manufacturer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a025b62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_useless = no_nulls.copy()\n",
    "values_to_replace = remove_useless.model.value_counts()[remove_useless.model.value_counts() < 1000].index\n",
    "remove_useless.loc[remove_useless.model.isin(values_to_replace), 'model'] = np.nan\n",
    "remove_useless = remove_useless.rename(columns={'size': 'size1'})\n",
    "remove_useless.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "manufacturer_dummies = pd.get_dummies(remove_useless.manufacturer, drop_first=True)\n",
    "model_dummies = pd.get_dummies(remove_useless.model, drop_first=True)\n",
    "condition_dummies = pd.get_dummies(remove_useless.condition, drop_first=True)\n",
    "cylinders_dummies = pd.get_dummies(remove_useless.cylinders, drop_first=True)\n",
    "fuel_dummies = pd.get_dummies(remove_useless.fuel, drop_first=True)\n",
    "title_status_dummies = pd.get_dummies(remove_useless.title_status, drop_first=True)\n",
    "transmission_dummies = pd.get_dummies(remove_useless.transmission, drop_first=True)\n",
    "drive_dummies = pd.get_dummies(remove_useless.drive, drop_first=True)\n",
    "size1_dummies = pd.get_dummies(remove_useless.size1, drop_first=True)\n",
    "type_dummies = pd.get_dummies(remove_useless.type, drop_first=True)\n",
    "paint_color_dummies = pd.get_dummies(remove_useless.paint_color, drop_first=True)\n",
    "final_df = pd.concat([remove_useless, manufacturer_dummies, model_dummies, condition_dummies, cylinders_dummies,\n",
    "                     fuel_dummies, title_status_dummies, transmission_dummies, drive_dummies, size1_dummies,\n",
    "                     type_dummies, paint_color_dummies], axis=\"columns\")\n",
    "final_df = final_df.drop([\"manufacturer\", \"model\", \"condition\", \"cylinders\", \"fuel\", \"title_status\", \"transmission\", \n",
    "                         \"drive\", \"size1\", \"type\", \"paint_color\"], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"please_god.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508311f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price\", axis=1)\n",
    "y = final_df.price\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  },\n",
    "  'suppor_vector_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', SVR())\n",
    "    ],\n",
    "    'params': {\n",
    "        'regressor__kernel': ['linear'],\n",
    "        'regressor__C': [10],\n",
    "        'regressor__epsilon': [0.1],\n",
    "        'regressor__gamma': ['auto']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27147bdd",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0721cf2",
   "metadata": {},
   "source": [
    "### Extracting meaningful words from name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d81f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_meaningful_words_df = no_outliers_df.copy()\n",
    "name_meaningful_words_df.name = name_meaningful_words_df.name.str.lower()\n",
    "# Extract number of bedrooms in a separate column\n",
    "name_meaningful_words_df[\"nr_of_bedrooms\"] = name_meaningful_words_df.name.str.extract('(\\d+)\\s*-?\\+?(?:br|bd|bed)')\n",
    "name_meaningful_words_df[\"has_nr_of_bedrooms\"] = ~name_meaningful_words_df[\"nr_of_bedrooms\"].isnull()\n",
    "name_meaningful_words_df[\"nr_of_bedrooms\"] = name_meaningful_words_df[\"nr_of_bedrooms\"].fillna(0)\n",
    "\n",
    "def most_frequent_words_in_name_column(dataframe):\n",
    "    # filter anything that is not helpful, or any words that are related to location\n",
    "    filtered_strings = {\"in\", \"private\", \"to\", \"bedroom\", \"the\", \"room\", \"home\", \"with\", \"apartment\", \"near\", \"and\", \"house\", \"of\", \n",
    "    \"a\", \"from\", \"bed\", \"apt\", \"bath\", \"close\", \"view\", \"suite\", \"on\", \"walk\", \"location\", \"for\", \"east\", \"by\", \"at\", \"hollywood\", \n",
    "    \"city\", \"one\", \"austin\", \"unit\", \"br\", \"nr\", \"west\", \"s\", \"d\", \"dt\", \"no\", \"bd\", \"b\", \"la\", \"full\", \"brooklyn\", \"san\", \"brand\", \n",
    "    \"hill\", \"steps\", \"minutes\", \"bathroom\", \"strip\", \"vegas\", \"manhattan\", \"prime\", \"las\", \"hills\", \"south\", \"mins\", \"hot\", \"guest\", \n",
    "    \"two\", \"free\", \"min\", \"hotel\", \"nyc\", \"place\", \"w\", \"floor\", \"stay\", \"nashville\", \"space\", \"shared\", \"away\", \"north\", \"entire\", \n",
    "    \"beds\", \"sleeps\", \"bay\", \"williamsburg\", \"side\", \"living\", \"kitchen\", \"rental\", \"square\", \"district\", \"located\", \"street\", \"area\", \"your\", \n",
    "    \"upper\", \"neighborhood\", \"pet\", \"st\", \"bdrm\", \"venice\", \"los\", \"style\", \"santa\", \"bedrooms\", \"blocks\", \"heights\", \"diego\", \n",
    "    \"block\", \"next\", \"miles\", \"capitol\", \"long\", \"centre\", \"dtla\", \"top\", \"broadway\", \"seattle\", \"mission\", \"all\"}\n",
    "\n",
    "    word_frequencies = dataframe.name.str.split('\\\\W+', expand=True).stack().value_counts()\n",
    "    word_frequencies = word_frequencies[(word_frequencies >= 1000) & word_frequencies.index.str.isalpha()]\n",
    "    word_frequencies = word_frequencies[~word_frequencies.index.isin(filtered_strings)]\n",
    "    \n",
    "    return word_frequencies\n",
    "\n",
    "most_frequent_words_in_name_column(name_meaningful_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8b2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequent words have to be manually extracted because similar words were grouped together to reduce number of columns\n",
    "# And some were removed due to not being useful\n",
    "meaningful_words_with_column_name = [\n",
    "    [\"peaceful\", [\"quiet\", \"oasis\", \"retreat\", \"getaway\", \"peaceful\"]],\n",
    "    [\"luxurios\", [\"luxury\", \"luxurious\", \"resort\", \"deluxe\"]],\n",
    "    [\"lovely\", [\"lovely\", \"charming\", \"cute\", \"nice\"]],\n",
    "    [\"spacious\", [\"spacious\", \"large\", \"huge\", \"big\"]],\n",
    "    [\"comfortable\", [\"cozy\", \"comfy\", \"comfortable\"]],\n",
    "    [\"central\", [\"downton\", \"heart\", \"central\", \"midtown\"]],\n",
    "    [\"amazing\", [\"great\", \"amazing\", \"best\", \"perfect\", \"paradise\"]],\n",
    "    [\"beautiful\", [\"beautiful\", \"gorgeous\", \"stunning\"]],\n",
    "    [\"bright\", [\"sunny\", \"bright\", \"cheerful\"]],\n",
    "    [\"renovated\", [\"renovated\", \"remodeled\", \"newly\"]],\n",
    "    [\"backyard\", [\"garden\", \"backyard\", \"yard\"]],\n",
    "    [\"beach\", [\"beach\", \"ocean\"]],\n",
    "    [\"pool\", [\"pool\", \"tub\"]],\n",
    "    [\"patio\", [\"deck\", \"patio\"]],\n",
    "    [\"stylish\", [\"stylish\", \"chic\"]],\n",
    "    [\"convenient\", [\"convenient\", \"everything\"]],\n",
    "    [\"townhouse\", [\"townhouse\", \"townhome\"]],\n",
    "    [\"urban\", [\"urban\", \"town\",]],\n",
    "    [\"modern\", [\"modern\", \"new\",]],\n",
    "    [\"village\", [\"village\"]],\n",
    "    [\"cottage\", [\"cottage\"]],\n",
    "    [\"studio\", [\"studio\"]],\n",
    "    [\"condo\", [\"condo\"]],\n",
    "    [\"bungalow\", [\"bungalow\"]],\n",
    "    [\"villa\", [\"villa\"]],\n",
    "    [\"penthouse\", [\"penthouse\"]],\n",
    "    [\"duplex\", [\"duplex\"]],\n",
    "    [\"loft\", [\"loft\"]],\n",
    "    [\"king\", [\"king\"]],\n",
    "    [\"queen\", [\"queen\"]],\n",
    "    [\"master\", [\"master\"]],\n",
    "    [\"gym\", [\"gym\"]],\n",
    "    [\"entrance\", [\"entrance\"]],\n",
    "    [\"balcony\", [\"balcony\"]],\n",
    "    [\"rooftop\", [\"rooftop\"]],\n",
    "    [\"mid\", [\"mid\"]],\n",
    "    [\"victorian\", [\"victorian\"]],\n",
    "    [\"historic\", [\"historic\"]],\n",
    "    [\"gem\", [\"gem\"]],\n",
    "    [\"clean\", [\"clean\"]],\n",
    "    [\"furnished\", [\"furnished\"]],\n",
    "    [\"family\", [\"family\"]],\n",
    "    [\"friendly\", [\"friendly\"]],\n",
    "    [\"views\", [\"views\"]],\n",
    "    [\"valley\", [\"valley\"]],\n",
    "    [\"park\", [\"park\"]],\n",
    "    [\"lake\", [\"lake\"]],\n",
    "    [\"waterfront\", [\"waterfront\"]],\n",
    "    [\"vacation\", [\"vacation\"]],\n",
    "    [\"spa\", [\"spa\"]],\n",
    "    [\"heated\", [\"heated\"]],\n",
    "    [\"airport\", [\"airport\"]],\n",
    "    [\"parking\", [\"parking\"]],\n",
    "    [\"wifi\", [\"wifi\"]]\n",
    "]\n",
    "\n",
    "# for each meaningful word column (column_name) create a new column in the dataframe with its name (e.g. luxurious)\n",
    "# split each row's name with delimiters being whitespace or special characters\n",
    "# if any word in the split name is inside a list of words (values_to_search)\n",
    "# set the value for the current column to True, otherwise set to False\n",
    "for column_name, values_to_search in meaningful_words_with_column_name:\n",
    "    name_meaningful_words_df[column_name] = name_meaningful_words_df.name.str.split('\\\\W+').apply(lambda cur_word: any(value_to_search in cur_word for value_to_search in values_to_search))\n",
    "\n",
    "name_meaningful_words_df = name_meaningful_words_df.drop(\"name\", axis=1)\n",
    "name_meaningful_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a21f20",
   "metadata": {},
   "source": [
    "### Checking if city column needs any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd439eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There aren't too many cities, so the values will remain unmodified until they are one-hot encoded\n",
    "name_meaningful_words_df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040321a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_frequency_values_removed_df = name_meaningful_words_df.copy()\n",
    "values_to_replace = reduced_frequency_values_removed_df.city.value_counts()[reduced_frequency_values_removed_df.city.value_counts() < 5000].index\n",
    "reduced_frequency_values_removed_df.loc[reduced_frequency_values_removed_df.city.isin(values_to_replace), 'city'] = 'Other'\n",
    "reduced_frequency_values_removed_df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f318483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are many values with a frequency of one, everything with a frequency < 1000 will be converted to 'Other'\n",
    "name_meaningful_words_df.neighbourhood.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_replace = reduced_frequency_values_removed_df.neighbourhood.value_counts()[reduced_frequency_values_removed_df.neighbourhood.value_counts() < 1000].index\n",
    "reduced_frequency_values_removed_df.loc[reduced_frequency_values_removed_df.neighbourhood.isin(values_to_replace), 'neighbourhood'] = 'Other'\n",
    "reduced_frequency_values_removed_df.neighbourhood.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e6299",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5328cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding the room_type, city and neighbourhood columns\n",
    "room_type_dummies = pd.get_dummies(reduced_frequency_values_removed_df.room_type, drop_first=True)\n",
    "city_dummies = pd.get_dummies(reduced_frequency_values_removed_df.city, drop_first=True)\n",
    "neighbourhood_dummies = pd.get_dummies(reduced_frequency_values_removed_df.neighbourhood, drop_first=True)\n",
    "final_df = pd.concat([reduced_frequency_values_removed_df, room_type_dummies, neighbourhood_dummies, city_dummies], axis=\"columns\")\n",
    "final_df = final_df.drop([\"neighbourhood\", \"room_type\", \"city\"], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c13e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe374e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6905a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['price_per_night'] = pd.cut(final_df['price_per_night'], bins=[-1, 90, 150, 257, float('inf')],\n",
    "                           labels=[0, 1, 2, 3])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92765de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"df_full_size.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446daa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df[final_df.has_nr_of_bedrooms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762e9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32acf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"loosing_my_neurons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df.price_per_night1.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956442e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LogisticRegression(max_iter=10000))\n",
    "    ],\n",
    "    'params': {}\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc6634",
   "metadata": {},
   "source": [
    "## Testing ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X = final_df.drop(\"price_per_night\", axis=1)\n",
    "y = final_df.price_per_night\n",
    "\n",
    "models = {\n",
    "  'linear_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', linear_model.LinearRegression())\n",
    "    ],\n",
    "    'params': {}\n",
    "  },\n",
    "  'suppor_vector_regression': {\n",
    "    'steps': [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('regressor', SVR())\n",
    "    ],\n",
    "    'params': {\n",
    "        'regressor__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'regressor__C': [0.1, 10],\n",
    "        'regressor__epsilon': [0.1, 0.001],\n",
    "        'regressor__gamma': ['scale', 'auto']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, options in models.items():\n",
    "    print(f\"checking model {model_name}\")\n",
    "    pipeline = Pipeline(options[\"steps\"])\n",
    "    grid_search = GridSearchCV(pipeline, options[\"params\"], cv=5, return_train_score=False, verbose = 4)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23a795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
