{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45755d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Airbnb-US-2023.csv\")\n",
    "# Give some columns clearer/shorter names\n",
    "df = df.rename(columns={\n",
    "    \"price\": \"price_per_night\",\n",
    "    \"number_of_reviews\": \"nr_of_reviews\",\n",
    "    \"last_review\": \"last_review_date\",\n",
    "    \"calculated_host_listings_count\": \"nr_of_listings_by_host\",\n",
    "    \"availability_365\": \"days_per_year_available\",\n",
    "    \"number_of_reviews_ltm\": \"nr_of_reviews_last_twelve_months\"\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34a3e3",
   "metadata": {},
   "source": [
    "## Visualizing the locations\n",
    "### Use the output of the function below with google my maps to plot a number of locations unto a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_locations_to_google_my_maps(dataframe, number_of_locations = 10000):\n",
    "    \"\"\"Extracts latitude, longitude, name, and room_type from the dataframe provided.\n",
    "    The locations are reduced until their number is close to number_of_locations.\n",
    "    The result is grouped based on room_type, each group is exported to multiple csv files of max length = 2000.\n",
    "    These are to be imported on Google My Maps for plotting.\"\"\"\n",
    "    \n",
    "    dataframe = dataframe[[\"latitude\", \"longitude\", \"name\", \"room_type\"]]\n",
    "    \n",
    "    entire_apartment_df = dataframe[dataframe[\"room_type\"] == \"Entire home/apt\"].drop(\"room_type\", axis=1)\n",
    "    private_room_df = dataframe[dataframe[\"room_type\"] == \"Private room\"].drop(\"room_type\", axis=1)\n",
    "    shared_room_df = dataframe[dataframe[\"room_type\"] == \"Shared room\"].drop(\"room_type\", axis=1)\n",
    "    hotel_room_df = dataframe[dataframe[\"room_type\"] == \"Hotel room\"].drop(\"room_type\", axis=1)\n",
    "    \n",
    "    locations_to_eliminate = len(dataframe) - number_of_locations\n",
    "    entire_apartment_ratio = len(entire_apartment_df) / len(dataframe)\n",
    "    private_room_ratio = len(private_room_df) / len(dataframe)\n",
    "    shared_room_ratio = len(shared_room_df) / len(dataframe)\n",
    "    hotel_room_ratio = len(hotel_room_df) / len(dataframe)\n",
    "    \n",
    "    entire_apartment_locations_to_delete = int(entire_apartment_ratio * locations_to_eliminate)\n",
    "    private_room_locations_to_delete = int(private_room_ratio * locations_to_eliminate)\n",
    "    shared_room_locations_to_delete = int(shared_room_ratio * locations_to_eliminate)\n",
    "    hotel_room_locations_to_delete = int(hotel_room_ratio * locations_to_eliminate)\n",
    "    \n",
    "    entire_apartment_locations_to_keep = len(entire_apartment_df) - entire_apartment_locations_to_delete\n",
    "    private_room_locations_to_keep = len(private_room_df) - private_room_locations_to_delete\n",
    "    shared_room_locations_to_keep = len(shared_room_df) - shared_room_locations_to_delete\n",
    "    hotel_room_locations_to_keep = len(hotel_room_df) - hotel_room_locations_to_delete\n",
    "    \n",
    "    entire_apartment_df_shortened = entire_apartment_df.sample(entire_apartment_locations_to_keep)\n",
    "    private_room_df_shortened = private_room_df.sample(private_room_locations_to_keep)\n",
    "    shared_room_df_shortened = shared_room_df.sample(shared_room_locations_to_keep)\n",
    "    hotel_room_df_shortened = hotel_room_df.sample(hotel_room_locations_to_keep)\n",
    "    \n",
    "    entire_apartment_dfs = np.array_split(entire_apartment_df_shortened, len(entire_apartment_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(entire_apartment_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/apartment{i}.csv\")\n",
    "    \n",
    "    private_room_dfs = np.array_split(private_room_df_shortened, len(private_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(private_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/private{i}.csv\")\n",
    "    \n",
    "    shared_room_dfs = np.array_split(shared_room_df_shortened, len(shared_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(shared_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/shared{i}.csv\")\n",
    "    \n",
    "    hotel_room_dfs = np.array_split(hotel_room_df_shortened, len(hotel_room_df_shortened) // 2000 + 1)\n",
    "    for i, df_chunk in enumerate(hotel_room_dfs):\n",
    "        df_chunk.to_csv(f\"google-my-maps-csvs/hotel{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55520",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "### Removing duplicates and irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length before removing irrelevant columns: {len(df)}\")\n",
    "relevant_df = df.drop([\"id\"], axis=1)\n",
    "relevant_df = relevant_df.drop_duplicates()\n",
    "# We drop neighbourhood_group because we already have the neighbourhood column which provides more information\n",
    "relevant_df = relevant_df.drop([\"name\", \"host_id\", \"host_name\", \"latitude\", \"longitude\", \"neighbourhood_group\"], axis=1)\n",
    "print(f\"Length after removing irrelevant columns: {len(relevant_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ad61b",
   "metadata": {},
   "source": [
    "### Dealing with missing values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ae120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56e0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The last_review_date values will be converted to unix timestamps\n",
    "# Since the timestamp being 0 isn't accurate, a has_reviews column will be added to the dataset to handle this scenario\n",
    "non_null_df = relevant_df.copy()\n",
    "non_null_df[\"has_reviews\"] = non_null_df.last_review_date.notnull().astype(int)\n",
    "non_null_df.last_review_date = non_null_df.last_review_date.fillna(0)\n",
    "non_null_df.reviews_per_month = non_null_df.reviews_per_month.fillna(0)\n",
    "non_null_df.loc[non_null_df.has_reviews == 1, \"last_review_date\"] = pd.to_datetime(non_null_df[non_null_df.has_reviews == 1].last_review_date).apply(lambda x: int(x.timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c4e66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of every column that could have outliers to see which ones have outliers\n",
    "# Alongside there will be plotted 2 vertical lines representing the bounds for eliminating outliers\n",
    "columns_used_for_checking_outliers = non_null_df.drop([\"neighbourhood\", \"room_type\", \"city\", \"has_reviews\"], axis=1).columns\n",
    "\n",
    "rows_of_subplots = int(len(columns_used_for_checking_outliers) / 2)\n",
    "columns_of_subplots = 2\n",
    "\n",
    "fig, axes = plt.subplots(rows_of_subplots, columns_of_subplots, figsize=(14, 10))\n",
    "fig.subplots_adjust(hspace=0.9, wspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for subplot_index, column_name in enumerate(columns_used_for_checking_outliers):\n",
    "    ax = axes[subplot_index]\n",
    "    ax.hist(non_null_df[column_name], bins=20, rwidth=0.8)\n",
    "    \n",
    "    mean = non_null_df[column_name].mean()\n",
    "    standard_deviation = non_null_df[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "\n",
    "    ax.axvline(x=lower_bound, color='b')\n",
    "    ax.axvline(x=upper_bound, color='b')\n",
    "    \n",
    "    # most subplots need to be scaled logarithmically because there is one range of values that is much more frequent\n",
    "    # than any other, thus the subplot won't give too much information unless it is scaled\n",
    "    if column_name != \"days_per_year_available\":\n",
    "        ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax.set_xlabel(column_name)\n",
    "    ax.set_ylabel(\"frequency\")\n",
    "    ax.set_title(f\"Distribution of {column_name}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_review_date and days_per_year_available will be excluded from outlier removal because they don't have any outliers\n",
    "\n",
    "columns_used_for_removing_outliers = non_null_df.drop([\"neighbourhood\", \"room_type\", \"city\", \"has_reviews\", \"last_review_date\", \"days_per_year_available\"], axis=1).columns\n",
    "\n",
    "print(f\"Length before removing outliers: {len(non_null_df)}\\n\")\n",
    "\n",
    "no_outliers_df = non_null_df.copy()\n",
    "\n",
    "for column_name in columns_used_for_removing_outliers:\n",
    "    mean = no_outliers_df[column_name].mean()\n",
    "    standard_deviation = no_outliers_df[column_name].std()\n",
    "    \n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    \n",
    "    percentage_removed = round((((no_outliers_df[column_name] < lower_bound) | (no_outliers_df[column_name] > upper_bound)).sum() / len(no_outliers_df)) * 100, 2)\n",
    "\n",
    "    print(f\"For column {column_name}, removing a percentage of {percentage_removed}% values.\")\n",
    "    no_outliers_df = no_outliers_df[(lower_bound <= no_outliers_df[column_name]) & (no_outliers_df[column_name] <= upper_bound)]\n",
    "\n",
    "print(f\"\\nLength after removing outliers: {len(no_outliers_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
